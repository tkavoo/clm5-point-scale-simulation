{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281223bb-463f-46c4-9eec-8950fb85ba4a",
   "metadata": {},
   "source": [
    "## Load Python Libraries\n",
    "\n",
    "Run the below code to import the required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009707a-9212-4223-a75d-d1d517217e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from glob import glob\n",
    "from os.path import join, expanduser\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from neon_utils import download_eval_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59316988-bdec-44a7-9b87-caad388c5963",
   "metadata": {},
   "source": [
    "## Open model data files\n",
    "\n",
    "Since our default run was a run till present, it has more than 36 months of output data. Therefore, we need to establish a way that reads data for 2018 to 2020 december. We are going to read the vcmax_min and vcmax_max output data directly as we did the run for only 36 months.\n",
    "\n",
    "Here we use the python function xarray.open_mfdataset, which opens multiple netcdf files as a single xarray dataset. For more information on this xarray function, visit the xarray website.\n",
    "\n",
    "Run the below cell to read in the data files. Note that this step might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334faf41-89ab-4e23-bdbe-0169ae650614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading specific years within the default run folder\n",
    "start = time.time()\n",
    "neon_site = \"TALL\"\n",
    "years = [\"2018\", \"2019\", \"2020\"]\n",
    "df_path = os.path.join('/home/user/archive/'+neon_site+'.transient/lnd/hist/')\n",
    "\n",
    "df_files = []\n",
    "for year in years:\n",
    "    df_file = join(df_path, 'TALL.transient.clm2.h0.'+year+'*.nc')\n",
    "    df_files += glob(df_file)\n",
    "\n",
    "ds_ctsm_def = xr.open_mfdataset(df_files, decode_times=True, combine='by_coords')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Reading all default run files took:\", end-start, \"s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e40ca4-8f57-4001-8069-afa38077ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we shall read our modified parameter model data\n",
    "# Remember the model prints both daily and monthly output files. We shall read the monthly output files \"H0\".\n",
    "# If you are interested in daily outputs, you can use \"h1\" in the cell below instead of \"h0\"\n",
    "ds_ctsm_vmin = xr.open_mfdataset(\"/home/user/archive/TALLvcmax_min/lnd/hist/TALLvcmax_min.clm2.h0.*.nc\")\n",
    "ds_ctsm_vmax = xr.open_mfdataset(\"/home/user/archive/TALLvcmax_max/lnd/hist/TALLvcmax_max.clm2.h0.*.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c95203-3854-469b-b2f7-055d27299ce7",
   "metadata": {},
   "source": [
    "## Downloading NEON data\n",
    "\n",
    "If you already downloaded the data, you can skip running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762675e1-2012-47fa-82fc-335149c61d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dir = \"/home/user/evaluation_files/\"\n",
    "download_eval_files(neon_site, eval_dir, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54df7ed-1931-409e-855f-3ea05712c63e",
   "metadata": {},
   "source": [
    "## Load NEON data\n",
    "\n",
    "Again, we downloaded all available neon data and we only need to extract data for 2018 to 2020, so we shall create a script to only read data for those specifi years. The code below does that and can be modified to read whatever years of data you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e49b8d7-b3d9-4321-94fe-122eafcf0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "years = [\"2018\", \"2019\", \"2020\"]\n",
    "eval_path = os.path.join('/home/user/evaluation_files/', neon_site)\n",
    "\n",
    "eval_files = []\n",
    "for year in years:\n",
    "    eval_file = join(eval_path, neon_site + \"_eval_\" + year + \"*.nc\")\n",
    "    eval_files += glob(eval_file)\n",
    "\n",
    "eval_files = sorted(eval_files)\n",
    "\n",
    "print(\"All Observation files:\")\n",
    "print(*eval_files, sep='\\n')\n",
    "\n",
    "ds_eval = xr.open_mfdataset(eval_files, decode_times=True, combine='by_coords')\n",
    "\n",
    "end = time.time()\n",
    "print(\"Reading all observation files took:\", end-start, \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b121680-fcbb-4c51-b5b5-e3c5ff516880",
   "metadata": {},
   "source": [
    "## Reading NEON Data and extracting for monthly\n",
    "\n",
    "The following script will help us read NEON data in monthly time steps for easy handling with the model we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c24fb8-b691-4d1c-b848-3cbe68d3dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_vars = ['EFLX_LH_TOT', 'GPP']\n",
    "df_eval = pd.DataFrame({'time':ds_eval.time})\n",
    "for var in eval_vars:\n",
    "    field = np.ravel(ds_eval[var])\n",
    "    df_eval[var] = field\n",
    "#print(\"All Observation files:\")\n",
    "#print(*eval_files,sep='\\n')\n",
    "df_eval['time'] = pd.to_datetime(df_eval['time'],format='%Y%m%d %H:%M')\n",
    "df_eval['year'] = pd.DatetimeIndex(df_eval['time']).year\n",
    "df_eval['month'] = pd.DatetimeIndex(df_eval['time']).month\n",
    "df_eval['day'] = pd.DatetimeIndex(df_eval['time']).day\n",
    "df_eval['hour'] = pd.DatetimeIndex(df_eval['time']).hour\n",
    "#-- extract year, month, day, hour information from time\n",
    "df_eval['year'] = df_eval['time'].dt.year\n",
    "df_eval['month'] = df_eval['time'].dt.month\n",
    "df_eval['day'] = df_eval['time'].dt.day\n",
    "df_eval['hour'] = df_eval['time'].dt.hour\n",
    "\n",
    "df_monthly = df_eval.groupby(['year','month']).mean().reset_index()\n",
    "df_monthly[\"day\"]=1\n",
    "df_monthly['time']=pd.to_datetime(df_monthly[[\"year\", \"month\",\"day\"]])\n",
    "\n",
    "df_eva = df_monthly\n",
    "ds_eva = df_eva.set_index('time').to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20b6c5-a581-4898-bed9-98d08791e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert CTSM data to a Pandas Dataframe for easier handling:\n",
    "ctsm_vars = ['FCEV', 'FCTR', 'FGEV','GPP']\n",
    "\n",
    "df_ctsm = pd.DataFrame({'time':ds_ctsm_def.time})\n",
    "\n",
    "for var in ctsm_vars:\n",
    "    field = np.ravel ( ds_ctsm_def[var])     \n",
    "    df_ctsm[var]=field\n",
    "\n",
    "df_ctsm_vmin = pd.DataFrame({'time':ds_ctsm_vmin.time})\n",
    "\n",
    "for var in ctsm_vars:\n",
    "    field = np.ravel ( ds_ctsm_vmin[var])     \n",
    "    df_ctsm_vmin[var]=field\n",
    "\n",
    "df_ctsm_vmax = pd.DataFrame({'time':ds_ctsm_vmax.time})\n",
    "\n",
    "for var in ctsm_vars:\n",
    "    field = np.ravel ( ds_ctsm_vmax[var])     \n",
    "    df_ctsm_vmax[var]=field\n",
    "\n",
    "#Convert NEON data to a Pandas Dataframe for easier handling:\n",
    "eval_vars = ['EFLX_LH_TOT','GPP']\n",
    "\n",
    "df_eval = pd.DataFrame({'time':ds_eva.time})\n",
    "\n",
    "for var in eval_vars:\n",
    "    field = np.ravel ( ds_eva[var])     \n",
    "    df_eval[var]=field\n",
    "\n",
    "#-- make df_all that includes both obs and sim\n",
    "df_all = df_eval\n",
    "\n",
    "#-- add simulation data to df_all and adjust for offset time dimension:\n",
    "for var in ctsm_vars:\n",
    "    sim_var_name = \"def_\"+var\n",
    "    #-- shift simulation data by one\n",
    "    df_all[sim_var_name]=df_ctsm[var].shift(-1).values\n",
    "\n",
    "for var in ctsm_vars:\n",
    "    sim_var_name1 = \"vmin_\"+var\n",
    "    #-- shift simulation data by one\n",
    "    df_all[sim_var_name1]=df_ctsm_vmin[var].shift(-1).values\n",
    "    \n",
    "for var in ctsm_vars:\n",
    "    sim_var_name2 = \"vmax_\"+var\n",
    "    #-- shift simulation data by one\n",
    "    df_all[sim_var_name2]=df_ctsm_vmax[var].shift(-1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc0e78-668e-47a8-90f6-881d2c562455",
   "metadata": {},
   "source": [
    "## Preparing and doing some conversion so that both the simulated and the observed data can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced96a5-0db4-47ec-9437-9a6103d4116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.fillna(0)    \n",
    "clm_var = 'def_EFLX_LH_TOT'\n",
    "\n",
    "#EFLX_LH_TOT = FCEV + FCTR +FGEV\n",
    "df_all [clm_var] = df_all['def_FCEV'] \\\n",
    "                 + df_all['def_FCTR']\\\n",
    "                 + df_all['def_FGEV']\n",
    "\n",
    "#Convert simulated GPP from gC to Umol Carbon\n",
    "df_all ['def_GPP'] = df_all['def_GPP'] * 83333.3\n",
    "\n",
    "clm_var1 = 'vmin_EFLX_LH_TOT'\n",
    "\n",
    "#EFLX_LH_TOT = FCEV + FCTR +FGEV\n",
    "df_all [clm_var1] = df_all['vmin_FCEV'] \\\n",
    "                 + df_all['vmin_FCTR']\\\n",
    "                 + df_all['vmin_FGEV']\n",
    "\n",
    "#Convert simulated GPP from gC to Umol Carbon\n",
    "df_all ['vmin_GPP'] = df_all['vmin_GPP'] * 83333.3\n",
    "\n",
    "clm_var2 = 'vmax_EFLX_LH_TOT'\n",
    "\n",
    "#EFLX_LH_TOT = FCEV + FCTR +FGEV\n",
    "df_all [clm_var2] = df_all['vmax_FCEV'] \\\n",
    "                 + df_all['vmax_FCTR']\\\n",
    "                 + df_all['vmax_FGEV']\n",
    "\n",
    "#Convert simulated GPP from gC to Umol Carbon\n",
    "df_all ['vmax_GPP'] = df_all['vmax_GPP'] * 83333.3\n",
    "\n",
    "df_allm = df_all.loc[:, ['EFLX_LH_TOT','GPP','def_EFLX_LH_TOT','def_GPP','vmin_EFLX_LH_TOT',\n",
    "             'vmin_GPP','vmax_EFLX_LH_TOT','vmax_GPP']]\n",
    "df_allm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664a40de-0c1e-4278-807a-37a295c1d0e6",
   "metadata": {},
   "source": [
    "## Visualizing the Latent Heat Flux and GPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073f834-e5a8-4dd2-8dd0-8a9b1bd9dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your data is stored in a DataFrame called 'df'\n",
    "gpp_cols = ['GPP', 'def_GPP', 'vmin_GPP', 'vmax_GPP']\n",
    "eflx_cols = ['EFLX_LH_TOT', 'def_EFLX_LH_TOT', 'vmin_EFLX_LH_TOT', 'vmax_EFLX_LH_TOT']\n",
    "\n",
    "# Filter the DataFrame to only include the desired columns\n",
    "gpp_data = df_allm[gpp_cols]\n",
    "eflx_data = df_allm[eflx_cols]\n",
    "\n",
    "# Set the index to the time column\n",
    "#gpp_data.set_index('time', inplace=True)\n",
    "#eflx_data.set_index('time', inplace=True)\n",
    "\n",
    "# Create the GPP plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "gpp_data.plot(ax=ax, kind='line', color=['black', 'red', 'green', 'blue'])\n",
    "ax.set_title('Gross Primary Productivity')\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('Gross Primary Productivity [Wm-2]')\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Create the EFLX_LH_TOT plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "eflx_data.plot(ax=ax, kind='line', color=['black', 'red', 'green', 'blue'])\n",
    "ax.set_title('Latent Heat Flux')\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('Latent Heat Flux [Wm-2]')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b9754-1443-4171-b879-15e7a3612b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
